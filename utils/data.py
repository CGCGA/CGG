import json
import os.path as osp
import os
import importlib

import networkx as nx
from rdkit import Chem
from rdkit.Chem import rdmolops
from torch.utils.data import Dataset
from typing import Union, List, Any, Optional, Dict
from abc import ABC, abstractmethod
from typing import Optional, Callable, Any
from collections.abc import Mapping
from lightning.pytorch import LightningDataModule
from torch.utils.data import DataLoader, RandomSampler, DistributedSampler
from torch.utils.data import Dataset
from torch_geometric.data import Dataset as PygDataset
from torch_geometric.loader import DataLoader as PygDataloader
from utils.dataset import DatasetWithCollate

import numpy as np
import torch
import torch_geometric as pyg
from torch_geometric.data import InMemoryDataset
import utils.model as model
from datasets import load_dataset

import pandas as pd
from utils.utils import get_label_texts

def safe_mkdir(path):
    if not osp.exists(path):
        os.mkdir(path)


def pth_safe_save(obj, path):
    if obj is not None:
        torch.save(obj, path)


def pth_safe_load(path):
    if osp.exists(path):
        return torch.load(path)
    return None

class OFAPygDataset(InMemoryDataset, ABC):
    r"""
    Base dataset class for OFA datasets. OFAPygDataset takes care of
    1, dataset loading
    2, text to feature transformation using LLM if specified.
    Currently, the class support two modes controlled by load_text. If load_text is true, the class
    only load raw texts into model. Otherwise, an LLM encoder must be specified to transform raw texts
    to feature.
    """

    def __init__(self, name: str, load_texts: bool, encoder: Optional[model.SentenceEncoder] = None,
                 root: str = "./datasets", transform: Optional[Callable] = None,
                 pre_transform: Optional[Callable] = None, ):

        self.name = name
        self.load_texts = load_texts
        self.root = root
        self.encoder = encoder
        if not self.load_texts:
            assert self.encoder is not None
            suffix = self.encoder.llm_name
        else:
            suffix = 'raw'
        self.data_dir = osp.join(self.root, self.name, suffix)

        super().__init__(self.data_dir, transform, pre_transform)
        safe_mkdir(self.data_dir)

        # # load text to the dataset instance
        # if self.load_texts:
        #     self.texts = torch.load(self.processed_paths[1])

        self.data, self.slices = torch.load(self.processed_paths[0])
        self.side_data = pth_safe_load(self.processed_paths[2])

    def data2vec(self, data: list[str]) -> torch.Tensor:
        r"""
        Encode a list of string to a len(data)-by-d matrix, where d is the output dimension of the LLM.
        """
        if self.encoder is None:
            raise NotImplementedError("LLM encoder is not defined")
        if data is None:
            return None
        embeddings = self.encoder.encode(data).cpu().numpy()
        return embeddings

    @property
    def num_classes(self):
        return self.__num_classes__

    @property
    def raw_file_names(self):
        return []

    @property
    def processed_file_names(self):
        return ["geometric_data_processed.pt", "texts.pkl", "data.pt"]

    def text2feature(self, texts):
        if isinstance(texts[0], str):
            return self.data2vec(texts)
        return [self.text2feature(t) for t in texts]

    @abstractmethod
    def gen_data(self) -> tuple[list[pyg.data.Data], list[list[str]], Any]:
        r"""
        Subclass should implement this method, it should generate
        1, a list of pyg.data.Data graphs
        2, a list of str sets that can be processed by self.text2feature
        3, any side data that should be stored during preprocessing
        The list of string (2) will be processed by encoder to vector representation and be fed into
        self.add_text_emb along with the list of graphs.
        """
        pass


    @abstractmethod
    def add_raw_texts(self, data_list, texts: list[str]) -> tuple[pyg.data.Data, Mapping]:
        r"""
        Args:
            data_list: a list of pyg.data.Data generated by self.gen_data
            texts: a list of text generated by self.gen_data

        Returns:

        """
        pass

    @abstractmethod
    def add_text_emb(self, data_list, texts_emb: list[torch.Tensor]) -> tuple[pyg.data.Data, Mapping]:
        r"""
        Args:
            data_list: a list of pyg.data.Data generated by self.gen_data
            texts_emb: a list of torch.Tensor generated by self.encoder from the text generated by self.gen_data

        Returns:

        """
        pass


    def process(self):

        data_list, texts, side_data = self.gen_data()

        torch.save(texts, self.processed_paths[1])
        if side_data is not None:
            torch.save(side_data, self.processed_paths[2])
        else:
            torch.save("No side data", self.processed_paths[2])

        if self.load_texts:
            data, slices = self.add_raw_texts(data_list, texts)
        else:
            if self.encoder.model is None:
                self.encoder.get_model()
            texts_emb = self.text2feature(texts)
            data, slices = self.add_text_emb(data_list, texts_emb)

        print("Saving...")
        torch.save((data, slices, ), self.processed_paths[0], pickle_protocol=4)

    @abstractmethod
    def get_task_map(self) -> dict[str, dict]:
        """
        :return: a task map specifying the text feature used by different tasks.
        """
        pass

    @abstractmethod
    def get_edge_list(self, mode="e2e") -> dict[str, list]:
        """
        Return the edge construction protocol for different tasks.
        Args:
            mode: a string representing the task

        Returns: a dictionary whose keys are the connection types including
            "f2n": feature to noi node
            "n2f": noi node to feature
            "n2c": noi node to class node
            "c2n": class node to noi node
        The values are lists of length 2. first element is the edge type, second element is
        the index to prompt_edge_text_feat.

        """
        pass

    def get_prompt_text_feat(self, task_name):
        """
        Return the list of prompt node/edge feature for the task.
        """
        task_map = self.get_task_map()
        if task_name not in task_map:
            raise NotImplementedError(
                "Task " + task_name + " is not implemented for " + self.name + " dataset the implemented tasks are "
                + str(
                    task_map.keys()))
        feat_ind = task_map[task_name]
        prompt_feats = {}
        for k in feat_ind:
            prompt_feats[k] = getattr(self.data, feat_ind[k][0])[feat_ind[k][1]]
        return prompt_feats

def gen_entities(name):
    if name == "WN18RR":
        entity2id = {}
        entity_lst = []
        text_lst = []
        with open(osp.join(osp.dirname(__file__), "dataelement/" + name, "entity2text.txt"), "r") as f:
            lines = f.readlines()
            for line in lines:
                tmp = line.strip().split("\t")
                entity_lst.append(tmp[0])
                text_lst.append(tmp[1])

        entity2id = {entity: i for i, entity in enumerate(entity_lst)}
    elif name == "FB15K237":
        entity_lst = []
        text_lst = []
        with open(osp.join(osp.dirname(__file__),"dataelement/" + name, "entity2wikidata.json"), "r") as f:
            data = json.load(f)

        for k in data:
            # print(data[k])
            entity_lst.append(k)
            text_lst.append("entity names: " + data[k]["label"] + ", entity alternatives: " + ", ".join(
                data[k]["alternatives"]) + ". entity descriptions:" + data[k]["description"] if data[k][
                                                                                                    "description"] is
                                                                                                not None else "None")

        entity2id = {entity: i for i, entity in enumerate(entity_lst)}
    else:
        raise NotImplementedError("Dataset " + name + " is not implemented.")
    return entity_lst, text_lst, entity2id


def read_knowledge_graph(files, name):
    entity_lst, text_lst, entity2id = gen_entities(name)
    relation2id = {}

    converted_triplets = {}
    rel_list = []
    rel = len(relation2id)

    for file_type, file_path in files.items():

        edges = []
        edge_types = []
        with open(file_path) as f:
            file_data = [line.split() for line in f.read().split("\n")[:-1]]
        unknown_entity = 0
        for triplet in file_data:
            if triplet[0] not in entity2id:
                text_lst.append("entity names: Unknown")
                entity_lst.append(triplet[0])
                entity2id[triplet[0]] = len(entity2id)
                unknown_entity += 1
            if triplet[2] not in entity2id:
                text_lst.append("entity names: Unknown")
                entity_lst.append(triplet[2])
                entity2id[triplet[2]] = len(entity2id)
                unknown_entity += 1
            if triplet[1] not in relation2id:
                relation2id[triplet[1]] = rel
                rel_list.append(triplet[1])
                rel += 1

            edges.append([entity2id[triplet[0]], entity2id[triplet[2]], ])
            edge_types.append(relation2id[triplet[1]])
        print(unknown_entity)
        converted_triplets[file_type] = [edges, edge_types]

    new_data = pyg.data.data.Data(x=torch.zeros([len(text_lst), 1]),
        edge_index=torch.tensor(converted_triplets["train"][0]).T,
        edge_types=torch.tensor(converted_triplets["train"][1]), )

    node_text = ["feature node. entity and entity description: " + ent for ent in text_lst]
    edge_text = ["feature edge. relation between two entities. " + relation for relation in rel_list] + [
        "feature edge. relation between two entities. the inverse relation of " + relation for relation in rel_list]

    # # different prompt edge feature for source and target node.
    prompt_edge_text = ["prompt edge.", "prompt edge connected with source node.", "prompt edge connected with target node.",
                        "prompt edge. edge for query graph that is our prediction target.",
                        "prompt edge. edge for support graph that is an example."]

    prompt_node_text = ["prompt node. relation type prediction between the connected entities.", ]
    label_text = ["prompt node. relation between two entities. " + relation for relation in rel_list]

    prompt_text_map = {"e2e_link": {"noi_node_text_feat": ["noi_node_text_feat", [0]],
                                    "class_node_text_feat": ["class_node_text_feat", torch.arange(len(label_text))],
                                    "prompt_edge_text_feat": ["prompt_edge_text_feat", [0, 1, 2]]},
                       "lr_link": {"noi_node_text_feat": ["noi_node_text_feat", [0]],
                                   "class_node_text_feat": ["class_node_text_feat", torch.arange(len(label_text))],
                                   "prompt_edge_text_feat": ["prompt_edge_text_feat", [0, 1, 2, 3, 4]]}}

    return ([new_data], [node_text, edge_text, label_text, prompt_edge_text, prompt_node_text, ],
            [converted_triplets, rel_list, prompt_text_map],)


class KGOFADataset(OFAPygDataset):
    def gen_data(self):
        cur_path = osp.dirname(__file__)
        cur_path += "/dataelement"
        print(cur_path)
        names = ["train", "valid", "test"]
        name_dict = {n: osp.join(cur_path, self.name, n + ".txt") for n in names}
        return read_knowledge_graph(name_dict, self.name)

    def add_raw_texts(self, data_list, texts):
        data_list[0].node_text_feat = np.array(texts[0])
        data_list[0].edge_text_feat = np.array(texts[1])
        data_list[0].class_node_text_feat = np.array(texts[2])
        data_list[0].prompt_edge_text_feat = np.array(texts[3])
        data_list[0].noi_node_text_feat = np.array(texts[4])
        return self.collate(data_list)

    def add_text_emb(self, data_list, text_emb):
        data_list[0].node_text_feat = text_emb[0]
        data_list[0].edge_text_feat = text_emb[1]
        data_list[0].class_node_text_feat = text_emb[2]
        data_list[0].prompt_edge_text_feat = text_emb[3]
        data_list[0].noi_node_text_feat = text_emb[4]
        return self.collate(data_list)

    def get_idx_split(self):
        return self.side_data[0]

    def get_task_map(self):
        return self.side_data[-1]

    def get_edge_list(self, mode="e2e"):
        if mode == "e2e_link":
            return {"f2n": [1, [1, 2]], "n2f": [3, [1, 2]], "n2c": [2, [0]], "c2n": [4, [0]]}
        elif mode == "lr_link":
            return {"f2n": [1, [0]], "n2f": [3, [0]]}
        
NAME_TO_SPLIT = {"chemblpre": "chembl_pretraining", "chempcba": "pcba", "chemhiv": "hiv"}


def load_prompt_json(name):
    if name == "chemblpre":
        with open(os.path.join(os.path.dirname(__file__), "dataelement/prompt_pretrain.json"), "rb") as f:
            prompt_text = json.load(f)
        return prompt_text["chembl"]
    else:
        with open(os.path.join(os.path.dirname(__file__), "dataelement/mol_label_desc.json"), "rb") as f:
            prompt_text = json.load(f)
        if name in NAME_TO_SPLIT:
            return prompt_text[NAME_TO_SPLIT[name]]
        else:
            raise NotImplementedError("Molecule dataset " + name + " not implemented.")


def get_local_text(name):
    print("gen text")
    cache_dir = os.path.join(os.path.dirname(__file__), "../dataset")
    data = load_dataset("haitengzhao/molecule_property_instruction", cache_dir=cache_dir, split=NAME_TO_SPLIT[name], )
    data_dict = {"label": data["label"], "task_index": data["task_index"], "molecule_index": data["molecule_index"], }
    pd_data = pd.DataFrame.from_dict(data_dict)
    cls_data = pd_data[np.logical_not(pd.isna(pd_data["task_index"]))]
    cls_data["ori_index"] = np.arange(len(cls_data))

    group = cls_data.groupby("molecule_index")
    index = group.ori_index.first()
    tasks = group.task_index.agg(lambda x: x.str.cat(sep=","))
    labels = group.label.agg(lambda x: x.str.cat(sep=","))
    mol = [data[i]["graph"] for i in index]
    split = [data[i]["split"] for i in index]

    prompt_text = load_prompt_json(name)
    task2index = {k: [i, prompt_text[k]] for i, k in enumerate(prompt_text)}
    label_text = get_label_texts(task2index)
    graphs = []
    for i in range(len(mol)):
        graph = smiles2graph(mol[i])
        task_lst = [task2index[v][0] for v in tasks[i].split(",")]
        label_lst = [1 if v == "Yes" else 0 for v in labels[i].split(",")]
        cur_label = np.zeros(len(task2index))
        cur_label[:] = np.nan
        cur_label[task_lst] = label_lst
        graph["label"] = cur_label
        graph["split"] = split[i]
        graphs.append(graph)
    return graphs, label_text


def gen_graph(graphs, labels_features):
    print("gen graph")

    node_texts = []
    edge_texts = []
    data = []
    for g in graphs:
        node_texts += g["node_feat"]
        edge_texts += g["edge_feat"]
    unique_node_texts = set(node_texts)
    unique_edge_texts = set(edge_texts)
    u_node_texts_lst = list(unique_node_texts)
    u_edge_texts_lst = list(unique_edge_texts)
    node_texts2id = {v: i for i, v in enumerate(u_node_texts_lst)}
    edge_texts2id = {v: i for i, v in enumerate(u_edge_texts_lst)}
    split = {"train": [], "valid": [], "test": []}
    for i, g in enumerate(graphs):
        cur_nt_id = [node_texts2id[v] for v in g["node_feat"]]
        cur_et_id = [edge_texts2id[v] for v in g["edge_feat"]]
        data.append(pyg.data.data.Data(x=torch.tensor(cur_nt_id, dtype=torch.long),
            xe=torch.tensor(cur_et_id, dtype=torch.long), edge_index=torch.tensor(g["edge_list"], dtype=torch.long).T,
            y=torch.tensor(g["label"]), ))
        split[g["split"]].append(i)

    prompt_edge_text = ["prompt edge.", "prompt edge. edge for query graph that is our target",
        "prompt edge. edge for support graph that is an example", ]
    prompt_text = ["prompt node. graph classification on molecule property",
        "prompt node. few shot task node for graph classification that decides whether the query molecule belongs to "
        "the class of support molecules.", ]

    prompt_text_map = {"e2e_graph": {"noi_node_text_feat": ["noi_node_text_feat", [0]],
                                     "class_node_text_feat": ["class_node_text_feat",
                                                              torch.arange(len(labels_features))],
                                     "prompt_edge_text_feat": ["prompt_edge_text_feat", [0]]},
                       "lr_graph": {"noi_node_text_feat": ["noi_node_text_feat", [1]],
                                    "class_node_text_feat": ["class_node_text_feat",
                                                             torch.arange(len(labels_features))],
                                    "prompt_edge_text_feat": ["prompt_edge_text_feat", [0, 1, 2]]}}

    ret = (data, [u_node_texts_lst, u_edge_texts_lst, labels_features, prompt_edge_text, prompt_text, ],
           [split, prompt_text_map],)
    return ret


class MolOFADataset(OFAPygDataset):
    def gen_data(self):
        pyg_graph, texts, split = gen_graph(*get_local_text(self.name))
        return [d for d in pyg_graph], texts, split

    def add_raw_texts(self, data_list, texts):
        data, slices = self.collate(data_list)
        data.node_embs = np.array(texts[0])
        data.edge_embs = np.array(texts[1])
        data.class_node_text_feat = np.array(texts[2])
        data.prompt_edge_text_feat = np.array(texts[3])
        data.noi_node_text_feat = np.array(texts[4])
        return data, slices

    def add_text_emb(self, data_list, text_emb):
        """
        Since the majority of node/edge text embeddings are repeated, we only store unique
        ones, and keep the indices.
        """
        data, slices = self.collate(data_list)
        data.node_embs = text_emb[0]
        data.edge_embs = text_emb[1]
        data.class_node_text_feat = text_emb[2]
        data.prompt_edge_text_feat = text_emb[3]
        data.noi_node_text_feat = text_emb[4]
        return data, slices

    def get(self, index):
        data = super().get(index)
        node_feat = self.node_embs[data.x.numpy()]
        edge_feat = self.edge_embs[data.xe.numpy()]
        data.node_text_feat = node_feat
        data.edge_text_feat = edge_feat
        data.y = data.y.view(1, -1)
        return data

    def get_idx_split(self):
        return self.side_data[0]

    def get_task_map(self):
        return self.side_data[1]

    def get_edge_list(self, mode="e2e"):
        if mode == "e2e_graph":
            return {"f2n": [1, [0]], "n2f": [3, [0]], "n2c": [2, [0]]}
            #return {"f2n": [1, [0]], "n2f": [2, [0]], "n2c": [3, [0]]}

        elif mode == "lr_graph":
            return {"f2n": [1, [0]], "n2f": [3, [0]]}


if __name__ == "__main__":
    g, label = gen_graph()
    print(g[0])

file1 = open(os.path.join(os.path.dirname(__file__), "dataelement/id2element.csv"), "r")
Lines = file1.readlines()
chem_dict = {}
for line in Lines:
    line_split = line.strip().split("\t")
    chem_dict[int(line_split[0])] = line_split[2]

allowable_features_map = {
    "possible_atomic_num_dict": chem_dict,
    "possible_chirality_dict": {
        "CHI_UNSPECIFIED": "unspecified",
        "CHI_TETRAHEDRAL_CW": "tetrahedral clockwise",
        "CHI_TETRAHEDRAL_CCW": "tetrahedral counter-clockwise",
        "CHI_OTHER": "other",
        "misc": "misc",
    },
    "possible_degree_list": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, "misc"],
    "possible_formal_charge_list": [
        -5,
        -4,
        -3,
        -2,
        -1,
        0,
        1,
        2,
        3,
        4,
        5,
        "misc",
    ],
    "possible_numH_list": [0, 1, 2, 3, 4, 5, 6, 7, 8, "misc"],
    "possible_number_radical_e_list": [0, 1, 2, 3, 4, "misc"],
    "possible_hybridization_list": [
        "SP",
        "SP2",
        "SP3",
        "SP3D",
        "SP3D2",
        "misc",
    ],
    "possible_is_aromatic_list": [False, True],
    "possible_is_in_ring_list": [False, True],
    "possible_bond_type_list": [
        "SINGLE",
        "DOUBLE",
        "TRIPLE",
        "AROMATIC",
        "misc",
    ],
    "possible_bond_stereo_dict": {
        "STEREONONE": "none",
        "STEREOZ": "Z",
        "STEREOE": "E",
        "STEREOCIS": "CIS",
        "STEREOTRANS": "TRANS",
        "STEREOANY": "ANY",
    },
    "possible_is_conjugated_list": [False, True],
}


def ReorderCanonicalRankAtoms(mol):
    order = tuple(
        zip(
            *sorted(
                [(j, i) for i, j in enumerate(Chem.CanonicalRankAtoms(mol))]
            )
        )
    )[1]
    mol_renum = Chem.RenumberAtoms(mol, order)
    return mol_renum, order


def get_chem_id2name():
    file1 = open("./id2element.csv", "r")
    Lines = file1.readlines()
    chem_dict = {}
    for line in Lines:
        line_split = line.strip().split(",")
        chem_dict[line_split[0]] = line_split[2]
    return chem_dict


def atom_to_feature(atom):
    """
    Converts rdkit atom object to feature list of indices
    :param mol: rdkit atom object
    :return: list
    """
    atom_feature = [
        chem_dict[int(atom.GetAtomicNum())],
        "atomic number is " + str(atom.GetAtomicNum()),
        allowable_features_map["possible_chirality_dict"][
            str(atom.GetChiralTag())
        ]
        + " chirality",
        "degree of " + str(atom.GetTotalDegree()),
        "formal charge of " + str(atom.GetFormalCharge()),
        "num of hydrogen is " + str(atom.GetTotalNumHs()),
        "num of radical electrons is " + str(atom.GetNumRadicalElectrons()),
        "hybridization is " + str(atom.GetHybridization()),
        "is aromatic" if atom.GetIsAromatic() else "not aromatric",
        "is in ring" if atom.IsInRing() else "not in ring",
    ]
    return "feature node. atom: " + " , ".join(atom_feature)


def bond_to_feature(bond):
    """
    Converts rdkit bond object to feature list of indices
    :param mol: rdkit bond object
    :return: list
    """
    bond_feature = [
        str(bond.GetBondType()) + " bond",
        "bond stereo is "
        + allowable_features_map["possible_bond_stereo_dict"][
            str(bond.GetStereo())
        ],
        "is conjugated" if bond.GetIsConjugated() else "not conjugated",
    ]
    return "feature edge. chemical bond. " + " , ".join(bond_feature)


def compute_cycle(mol):
    cycle_list = nx.cycle_basis(nx.Graph(rdmolops.GetAdjacencyMatrix(mol)))
    if len(cycle_list) == 0:
        cycle_length = 0
    else:
        cycle_length = max([len(j) for j in cycle_list])
    if cycle_length <= 6:
        cycle_length = 0
    else:
        cycle_length = cycle_length - 6
    cycle_score = -cycle_length
    return cycle_score

def smiles2graph(smiles_string, removeHs=True, reorder_atoms=False):
    """
    Converts SMILES string to graph Data object
    :input: SMILES string (str)
    :return: graph object
    """

    mol = Chem.MolFromSmiles(smiles_string)
    cycle_score = compute_cycle(mol)
    mol = mol if removeHs else Chem.AddHs(mol)
    if reorder_atoms:
        mol, _ = ReorderCanonicalRankAtoms(mol)

    # atoms
    atom_features_list = []
    for atom in mol.GetAtoms():
        atom_features_list.append(atom_to_feature(atom))

    # bonds
    edges_list = []
    edge_features_list = []
    bonds = mol.GetBonds()
    if len(bonds) == 0:
        edge_list = np.zeros((0, 2))
    else:
        for bond in mol.GetBonds():
            i = bond.GetBeginAtomIdx()
            j = bond.GetEndAtomIdx()

            edge_feature = bond_to_feature(bond)

            # add edges in both directions
            edges_list.append((i, j))
            edge_features_list.append(edge_feature)
            edges_list.append((j, i))
            edge_features_list.append(edge_feature)
        edge_list = np.array(edges_list)

    graph = dict()
    graph["edge_list"] = edge_list
    graph["edge_feat"] = edge_features_list
    graph["node_feat"] = atom_features_list
    graph["cycle"] = cycle_score

    return graph


AVAILABLE_DATA = ["Cora", "Pubmed", "wikics", "arxiv"]


class SingleGraphOFADataset(OFAPygDataset):
    def gen_data(self):
        if self.name not in AVAILABLE_DATA:
            raise NotImplementedError("Data " + self.name + " is not implemented")
        data_module = importlib.import_module("utils.single_graph." + self.name + ".gen_data")
        return data_module.get_data(self)

    def add_raw_texts(self, data_list, texts):
        data_list[0].node_text_feat = np.array(texts[0])
        data_list[0].edge_text_feat = np.array(texts[1])
        data_list[0].noi_node_text_feat = np.array(texts[2])
        data_list[0].class_node_text_feat = np.array(texts[3])
        data_list[0].prompt_edge_text_feat = np.array(texts[4])
        return self.collate(data_list)

    def add_text_emb(self, data_list, text_emb):
        data_list[0].node_text_feat = text_emb[0]
        data_list[0].edge_text_feat = text_emb[1]
        data_list[0].noi_node_text_feat = text_emb[2]
        data_list[0].class_node_text_feat = text_emb[3]
        data_list[0].prompt_edge_text_feat = text_emb[4]
        return self.collate(data_list)

    def get_task_map(self):
        return self.side_data

    def get_edge_list(self, mode="e2e"):
        if mode == "e2e_node":
            return {"f2n": [1, [0]],
                    "n2f": [3, [0]],
                    "n2c": [2, [0]],
                    "c2n": [4, [0]]
                    }
        elif mode == "lr_node":
            return {"f2n": [1, [0]],
                    "n2f": [3, [0]],
                    }
        elif mode == "e2e_link":
            return {"f2n": [1, [0]], "n2f": [3, [0]], "n2c": [2, [0]], "c2n": [4, [0]]}
        
class DataWithMeta:
    def __init__(
            self,
            data: Dataset,
            batch_size: int,
            state_name: Optional[str] = None,
            feat_dim: int = 0,
            metric: Optional[str] = None,
            classes: Union[int, List[int]] = 2,
            is_regression: bool = False,
            meta_data: Any = None,
            sample_size: Optional[int] = -1,
    ):
        self.data = data
        self.batch_size = batch_size
        self.state_name = state_name
        self.feat_dim = feat_dim
        self.meta_data = meta_data
        self.metric = metric
        self.sample_size = sample_size
        self.classes = classes
        if isinstance(classes, list):
            self.num_tasks = len(classes)
        else:
            self.num_tasks = None
        self.is_regression = is_regression

    def pred_dim(self):
        if self.is_regression:
            return 1
        if self.num_tasks is not None:
            return self.num_tasks
        return self.classes
    
class DataModule(LightningDataModule):
    def __init__(
            self,
            data: Dict[str, DataWithMeta],
            gpu_size=1,
            num_workers: int = 4,
            pin_memory=True,
    ):
        super().__init__()
        self.datasets = data
        self.gpu_size = gpu_size
        self.num_workers = num_workers
        self.pin_memory = pin_memory

    def create_dataloader(
            self,
            data: Dataset,
            sample_size: int,
            batch_size: int,
            drop_last: bool = True,
            shuffle: bool = True,
            num_workers: int = 0,
    ):
        # Adding distributed sampler for multi-GPU parallel training if number of GPU larger than one.
        # At this time, sample_size does not have any effect.
        sampler = None
        loader_shuffle = shuffle
        if self.gpu_size > 1:
            sampler = DistributedSampler(data, num_replicas=self.gpu_size, shuffle=shuffle)
            loader_shuffle = False
        else:
            if sample_size > 0:
                sampler = RandomSampler(data, num_samples=sample_size, replacement=True)
                loader_shuffle = False

        if isinstance(data, DatasetWithCollate):
            return DataLoader(
                data,
                batch_size,
                sampler=sampler,
                shuffle=loader_shuffle,
                num_workers=num_workers,
                collate_fn=data.get_collate_fn(),
                drop_last=drop_last,
                pin_memory=self.pin_memory,
            )
        if isinstance(data, PygDataset):
            return PygDataloader(
                data,
                batch_size,
                shuffle=loader_shuffle,
                sampler=sampler,
                num_workers=num_workers,
                collate_fn=data.get_collate_fn(),
                drop_last=drop_last,
                pin_memory=self.pin_memory,
            )

    def train_dataloader(self):
        return self.create_dataloader(
            self.datasets["train"].data,
            self.datasets["train"].sample_size,
            self.datasets["train"].batch_size,
            num_workers=self.num_workers,
        )

    def val_dataloader(self):
        if isinstance(self.datasets["val"], list):
            data_list = []
            for val_data in self.datasets["val"]:
                data_list.append(
                    self.create_dataloader(
                        val_data.data,
                        val_data.sample_size,
                        val_data.batch_size,
                        drop_last=False,
                        shuffle=False,
                        num_workers=self.num_workers,
                    )
                )
            return data_list
        else:
            return [
                self.create_dataloader(
                    self.datasets["val"].data,
                    self.datasets["val"].sample_size,
                    self.datasets["val"].batch_size,
                    drop_last=False,
                    shuffle=False,
                    num_workers=self.num_workers,
                )
            ]

    def test_dataloader(self):
        if isinstance(self.datasets["test"], list):
            data_list = []
            for test_data in self.datasets["test"]:
                data_list.append(
                    self.create_dataloader(
                        test_data.data,
                        test_data.sample_size,
                        test_data.batch_size,
                        drop_last=False,
                        shuffle=False,
                        num_workers=self.num_workers,
                    )
                )
            return data_list
        else:
            return [
                self.create_dataloader(
                    self.datasets["test"].data,
                    self.datasets["test"].sample_size,
                    self.datasets["test"].batch_size,
                    drop_last=False,
                    shuffle=False,
                    num_workers=self.num_workers,
                )
            ]